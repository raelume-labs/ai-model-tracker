# Language Model Tracker

Tracking multimodal language models, text generation models, and reasoning models.

## Models Overview

| Model | Company | Release Date | Size | Key Features | Benchmarks |
|-------|---------|--------------|------|--------------|------------|
| Gemini 3.1 Pro | Google DeepMind | Feb 19, 2026 | Pro-tier | 1M-token context, 77.1% ARC-AGI-2, multimodal reasoning, agentic coding capabilities | 77.1% ARC-AGI-2 benchmark |
| Qwen3.5 | Alibaba | Feb 16, 2026 | Multiple sizes | Native multimodal (text/image/video), agentic AI optimization, improved cost efficiency | TBD |
| Kimi K2.5 | Moonshot AI | Jan 26, 2026 | Open-weight | Multimodal LLM, vision + coding, agent swarm capabilities, 15T token pretraining | Comparable to GPT-5/Gemini on coding |
| Claude Opus 4.6 | Anthropic | Feb 2026 | - | High-reasoning, multi-source analysis, technical domain optimization | 68% vs 58% baseline (Box eval) |
| Claude Sonnet 4.6 | Anthropic | Feb 2026 | - | Balanced performance/cost, multimodal capabilities | TBD |
| Grok 4.1 Fast | xAI (Elon Musk) | Feb 21, 2026 | - | Enterprise-focused, integrated in Microsoft Copilot Studio | TBD |
| Doubao 2.0 | ByteDance | Feb 2026 | - | Multimodal language model, video understanding | TBD |

## Recent Updates

### February 2026

**Gemini 3.1 Pro (Feb 19, 2026) - Google DeepMind**
- Advanced Pro-tier model with 1M-token context window
- 77.1% performance on ARC-AGI-2 benchmark (significant jump from baseline)
- Multimodal reasoning across text, images, audio, video, and code
- Integrated into GitHub Copilot with focus on agentic coding

**Qwen3.5 (Feb 16, 2026) - Alibaba**
- Native multimodal capabilities (text, image, video)
- Built for "agentic AI era" with focus on autonomous task execution
- Improved performance per unit of inference cost
- Multiple size variants available

**Claude Opus 4.6 & Sonnet 4.6 (Feb 2026) - Anthropic**
- Opus 4.6: Specialized for high-reasoning tasks, 10% performance lift in technical domains
- Sonnet 4.6: Balanced option with improved cost efficiency
- Both feature enhanced multimodal understanding

**Grok 4.1 Fast (Feb 21, 2026) - xAI**
- Released in Microsoft Copilot Studio integration
- Focused on enterprise workflow automation
- Follows xAI's pattern of business-first deployments

### January 2026

**Kimi K2.5 (Jan 26, 2026) - Moonshot AI**
- Open-weight multimodal LLM with 15 trillion token pretraining
- Native multimodal architecture (not stitched components)
- Coding with vision capabilities
- Agent swarm execution for complex tasks
- Performance comparable to GPT-5 and Gemini on coding benchmarks

## Model Categories

### Reasoning-Focused
- Claude Opus 4.6: Multi-source analysis across legal, financial, technical content
- Qwen3.5: Agentic task execution optimization

### Multimodal
- Kimi K2.5: Native vision + text training
- Qwen3.5: Text, image, and video understanding
- Claude models: Enhanced multimodal capabilities

### Open-Weight
- Kimi K2.5: Available on HuggingFace and NVIDIA NIM

### Commercial/Closed
- Qwen3.5: Alibaba Cloud API
- Claude models: Anthropic API

## Benchmark Performance

### Coding Benchmarks
- **Kimi K2.5**: Comparable to GPT-5 and Gemini on coding tasks
- Specific metrics TBD pending public benchmarks

### Reasoning Benchmarks
- **Claude Opus 4.6**: 68% performance vs 58% baseline (Box evaluation)
- Near-perfect scores in technical domains

### Cost Efficiency
- **Qwen3.5**: Optimized for capability per unit of inference cost
- **Claude Sonnet 4.6**: Balanced performance/cost option

*Last updated: February 19, 2026*